{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "import numpy as np\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(file_path: str) -> list[str]:\n",
    "    try:\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            return f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"File not found. Please check the file path and try again.\")\n",
    "        return []\n",
    "\n",
    "def to_pairs(doc: list) -> list[list[str]]:\n",
    "    paired = [line.strip().split(\"\\t\") for line in doc]\n",
    "    return paired\n",
    "\n",
    "def clean_pairs(lines: list) -> list[list[str]]:\n",
    "    cleaned = list()\n",
    "    re_print = re.compile(\"[^%s]\" % re.escape(string.printable))\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            line = normalize(\"NFD\", line).encode(\"ascii\", \"ignore\")\n",
    "            line = line.decode(\"UTF-8\")\n",
    "            line = line.split()\n",
    "            line = [word.lower() for word in line]\n",
    "            line = [word.translate(table) for word in line]\n",
    "            line = [re_print.sub(\"\", w) for w in line]\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            clean_pair.append(\" \".join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return np.array(cleaned)\n",
    "\n",
    "def save_clean_data(sentences: list, filename: str) -> None:\n",
    "    dump(sentences, open(filename, \"wb\"))\n",
    "    print(f\"Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: dataset/en-de.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = \"dataset/en-de.txt\"\n",
    "doc = load_doc(filename)\n",
    "pairs = to_pairs(doc)\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "save_clean_data(clean_pairs, \"dataset/en-de.pkl\")\n",
    "\n",
    "# show the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load \n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_data(filename: str) -> list:\n",
    "    return load(open(filename, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_clean_data(\"dataset/en-de.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152820"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: dataset/en-de-both.pkl\n",
      "Saved: dataset/en-de-train.pkl\n",
      "Saved: dataset/en-de-test.pkl\n"
     ]
    }
   ],
   "source": [
    "# reduce dataset size\n",
    "\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "\n",
    "# split to train/set\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "\n",
    "save_clean_data(dataset, \"dataset/en-de-both.pkl\")\n",
    "save_clean_data(train, \"dataset/en-de-train.pkl\")\n",
    "save_clean_data(test, \"dataset/en-de-test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 9000, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, train, test = load_clean_data(\"dataset/en-de-both.pkl\"), load_clean_data(\"dataset/en-de-train.pkl\"), load_clean_data(\"dataset/en-de-test.pkl\")\n",
    "\n",
    "len(dataset), len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def get_maxlen(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2404\n",
      "English Max Length: 5\n"
     ]
    }
   ],
   "source": [
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = get_maxlen(dataset[:, 0])\n",
    "\n",
    "print(f\"English Vocabulary Size: {eng_vocab_size}\")\n",
    "print(f\"English Max Length: {eng_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Vocabulary Size: 3856\n",
      "German Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "# German Tokenizer\n",
    "\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = get_maxlen(dataset[:, 1])\n",
    "\n",
    "print(f\"German Vocabulary Size: {ger_vocab_size}\")\n",
    "print(f\"German Max Length: {ger_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    X = pad_sequences(X, maxlen=length, padding=\"post\")\n",
    "    return X\n",
    "\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "x_train = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "y_train = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "y_train = encode_output(y_train, ger_vocab_size)\n",
    "# prepare validation data\n",
    "x_test= encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "y_test = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "y_test = encode_output(y_test, ger_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 5, 256)            615424    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " repeat_vector_1 (RepeatVec  (None, 10, 256)           0         \n",
      " tor)                                                            \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 10, 256)           525312    \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 10, 3856)          990992    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2657040 (10.14 MB)\n",
      "Trainable params: 2657040 (10.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, RepeatVector, Dense, TimeDistributed\n",
    "\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation=\"softmax\")))\n",
    "    return model\n",
    "\n",
    "model = define_model(eng_vocab_size, ger_vocab_size, eng_length, ger_length, 256)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "282/282 [==============================] - 36s 127ms/step - loss: 2.1371 - val_loss: 2.1187\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 33s 116ms/step - loss: 1.9997 - val_loss: 2.0123\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 32s 114ms/step - loss: 1.8673 - val_loss: 1.9291\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 33s 116ms/step - loss: 1.7388 - val_loss: 1.8187\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 33s 116ms/step - loss: 1.6061 - val_loss: 1.7471\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 33s 115ms/step - loss: 1.4976 - val_loss: 1.6740\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 33s 115ms/step - loss: 1.3936 - val_loss: 1.6212\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 33s 116ms/step - loss: 1.2994 - val_loss: 1.5756\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 33s 116ms/step - loss: 1.2095 - val_loss: 1.5302\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 33s 118ms/step - loss: 1.1255 - val_loss: 1.4951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x132f4324090>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "# Train the model with the callback to log data for TensorBoard\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val), callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdulmunim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model.h5\")\n",
    "dump(eng_tokenizer, open(\"eng_tokenizer.pkl\", \"wb\"))\n",
    "dump(ger_tokenizer, open(\"ger_tokenizer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine translation, generate text \n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model(\"model.h5\")\n",
    "english_tokenizer = load(open(\"eng_tokenizer.pkl\", \"rb\"))\n",
    "german_tokenizer = load(open(\"ger_tokenizer.pkl\", \"rb\"))\n",
    "\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [np.argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return \" \".join(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate(model, tokenizer, source, source_tokenizer):\n",
    "    source = encode_sequences(source_tokenizer, eng_length, source)\n",
    "    translation = predict_sequence(model, tokenizer, source)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(loaded_model, german_tokenizer, \"Hi How are you?\", english_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
